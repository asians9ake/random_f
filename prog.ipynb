{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "271b13a4-fa3d-4787-8f44-1ca6c6139cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words: ['is', 'a', 'of']\n",
      "POS Tags of stop Words: [('is', 'VBZ'), ('a', 'DT'), ('of', 'IN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ashutosh\n",
      "[nltk_data]     Singhania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "text=\"Natural Language Processing is a fascinating field of AI.\" \n",
    "tokens= word_tokenize(text) \n",
    "stop_words=set(stopwords.words('english')) \n",
    "stop_words_tokens=[word for word in tokens if word.lower() in stop_words] \n",
    "print(\"stop_words:\", stop_words_tokens) \n",
    "pos_tags =nltk.pos_tag(stop_words_tokens) \n",
    "print(\"POS Tags of stop Words:\", pos_tags) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193568cf-c14d-40d7-8252-f45013bdb9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['amazing' 'and' 'document' 'find' 'go' 'hand' 'helps' 'idf' 'important'\n",
      " 'in' 'is' 'language' 'learning' 'machine' 'natural' 'nlp' 'processing'\n",
      " 'tf' 'words']\n",
      "TF-IDF Matrix\n",
      " [[0.4472136  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.4472136  0.4472136\n",
      "  0.         0.         0.4472136  0.         0.4472136  0.\n",
      "  0.        ]\n",
      " [0.         0.32311233 0.         0.         0.32311233 0.64622465\n",
      "  0.         0.         0.         0.24573525 0.         0.\n",
      "  0.32311233 0.32311233 0.         0.32311233 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.36325471 0.36325471 0.         0.\n",
      "  0.36325471 0.36325471 0.36325471 0.27626457 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.36325471\n",
      "  0.36325471]]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "docs= [\"Natural Language processing is amazing.\", \n",
    "       \"Machine learning and NLP go hand in hand.\", \n",
    "       \"TF-IDF helps find important words in a document.\"] \n",
    "vec= TfidfVectorizer() \n",
    "tfidf_mat= vec.fit_transform(docs) \n",
    "print(\"Feature Names:\", vec.get_feature_names_out()) \n",
    "print(\"TF-IDF Matrix\\n\", tfidf_mat.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7508212-324f-47ba-a312-38934d949f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams \n",
    "from collections import Counter, defaultdict \n",
    "txt= \"Natural languga eprocessing s fun and challenging.\" \n",
    "N=2 \n",
    "token= txt.lower().split() \n",
    "n_grams=list(ngrams(token,N)) \n",
    "model=defaultdict(lambda:0) \n",
    "counts=Counter(n_grams) \n",
    "for ngram, count in counts.items(): \n",
    "    prefix=ngram[:-1] \n",
    "    total_prefix_counts=sum(c for ng,c in counts.items() \n",
    "                            if ng[:-1] == prefix\n",
    "                           ) \n",
    "    model[ngram] =count/total_prefix_counts \n",
    "print(\"N-gram Probabilities:\", dict(model)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ecb66-d240-452d-b1d6-e0a7427f655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec \n",
    "sent=[[\"natural\",\"language\",\"processing\",\"is\",\"fun\"], \n",
    "      [\"machine\",\"learning\",\"and\",\"nlp\",\"go\",\"together\"]] \n",
    "model= Word2Vec(sent, vector_size=50,window=3,min_count=1,sg=1) \n",
    "vector=model.wv[\"language\"] \n",
    "print(\"word Vector for 'language':\", vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e966a9-44e8-400b-af3d-d5387941fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "corpus = [\n",
    "    \"I love this movie\",\n",
    "    \"This movie is amazing\",\n",
    "    \"I really enjoyed this film\",\n",
    "    \"What a fantastic experience\",\n",
    "    \"This movie is a masterpiece\"\n",
    "]\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "max_seq_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 64, input_length=max_seq_length-1),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "\n",
    "def predict_next_words(model, tokenizer, text, n=1, temperature=1.0):\n",
    "    for _ in range(n):\n",
    "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_seq_length-1, padding='pre')\n",
    "        predictions = model.predict(sequence, verbose=0)\n",
    "        \n",
    "        \n",
    "        predictions = np.log(predictions + 1e-7) / temperature\n",
    "        probabilities = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "        \n",
    "        predicted_index = np.random.choice(range(total_words), p=probabilities[0])\n",
    "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "        \n",
    "       \n",
    "        if output_word and (output_word not in text.split()[-2:]):\n",
    "            text += \" \" + output_word\n",
    "    return text\n",
    "\n",
    "# Test the model\n",
    "seed_text = \"I really enjoyed \"\n",
    "next_words = 3\n",
    "generated_text = predict_next_words(model, tokenizer, seed_text, next_words, temperature=0.8)\n",
    "print(f\"Input Text: {seed_text}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01afb9c-25bd-4f35-be05-74d424f861dc",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer \n",
    " \n",
    "EPOCHS = 15 \n",
    "BATCH_SIZE = 8 \n",
    "LEARNING_RATE = 5e-6 \n",
    "MAX_LENGTH = 75 \n",
    " \n",
    "class TextDataset(Dataset): \n",
    "    def __init__(self, text, tokenizer, max_length): \n",
    "        self.input_ids = [] \n",
    "        self.attn_masks = [] \n",
    "        for line in text: \n",
    "            encodings_dict = tokenizer(line, truncation=True, max_length=max_length, padding=\"max_length\") \n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids'])) \n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'])) \n",
    " \n",
    "    def __len__(self): \n",
    "        return len(self.input_ids) \n",
    " \n",
    "    def __getitem__(self, idx): \n",
    "        return self.input_ids[idx], self.attn_masks[idx] \n",
    " \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') \n",
    "if tokenizer.pad_token is None: \n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'}) \n",
    " \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2') \n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    " \n",
    "text_data = [ \n",
    "    \"The quick brown fox jumps over the lazy dog.\", \n",
    "    \"The sun sets in the west and rises in the east.\", \n",
    "    \"Artificial Intelligence is transforming the world.\", \n",
    "    \"Deep learning models are revolutionizing various industries.\", \n",
    "    \"Natural Language Processing is a key area of artificial intelligence.\", \n",
    "    \"Machine learning models are data-driven and improve over time.\", \n",
    "    \"The future of technology lies in autonomous systems and robotics.\", \n",
    "    \"Cloud computing has become the backbone of modern infrastructure.\" \n",
    "] \n",
    "dataset = TextDataset(text_data, tokenizer, max_length=MAX_LENGTH) \n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    " \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE) \n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = model.to(device) \n",
    "model.train() \n",
    " \n",
    "for epoch in range(EPOCHS): \n",
    "    for batch in dataloader: \n",
    "        input_ids, attn_masks = [x.to(device) for x in batch] \n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(input_ids, attention_mask=attn_masks, labels=input_ids) \n",
    "        loss = outputs.loss \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss.item()}\") \n",
    " \n",
    "model.eval() \n",
    "prompt = \"Artificial Intelligence\" \n",
    "encoded_input = tokenizer(prompt, return_tensors='pt', padding=True).to(device) \n",
    "generated_ids = model.generate(encoded_input['input_ids'], max_length=MAX_LENGTH, \n",
    "num_return_sequences=1, pad_token_id=tokenizer.pad_token_id) \n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True) \n",
    " \n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f432f9f-2a14-4b4c-b225-51f02bfd3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn \n",
    " \n",
    "# Parameters \n",
    "SEQ_LEN = 5 \n",
    "D_MODEL = 4 \n",
    " \n",
    "# Sample Sequence Dataset \n",
    "sequence = torch.tensor([[1.0, 0.0, 1.0, 0.0], \n",
    "                         [0.0, 2.0, 0.0, 1.0], \n",
    "                         [1.0, 1.0, 1.0, 1.0], \n",
    "                         [0.0, 0.0, 2.0, 1.0], \n",
    "                         [1.0, 2.0, 0.0, 0.0]]) \n",
    " \n",
    "# Self-Attention Components \n",
    "class SelfAttention(nn.Module): \n",
    "    def __init__(self, d_model): \n",
    "        super(SelfAttention, self).__init__() \n",
    "        self.query = nn.Linear(d_model, d_model) \n",
    "        self.key = nn.Linear(d_model, d_model) \n",
    "        self.value = nn.Linear(d_model, d_model) \n",
    " \n",
    "    def forward(self, x): \n",
    "        Q = self.query(x) \n",
    "        K = self.key(x) \n",
    "        V = self.value(x) \n",
    " \n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(K.size(-1)) \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1) \n",
    "        attention_output = torch.matmul(attention_weights, V) \n",
    "        return attention_output, attention_weights \n",
    " \n",
    "# Initialize Self-Attention \n",
    "self_attention = SelfAttention(D_MODEL) \n",
    " \n",
    "# Compute Attention Outputs and Weights \n",
    "attention_output, attention_weights = self_attention(sequence) \n",
    " \n",
    "# Visualize Attention Map \n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.imshow(attention_weights.detach().numpy(), cmap=\"viridis\") \n",
    "plt.colorbar() \n",
    "plt.title(\"Attention Map\") \n",
    "plt.xlabel(\"Key Positions\") \n",
    "plt.ylabel(\"Query Positions\") \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
